{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name : 'Muhammad Nouman Butt' & 'Rafaqat Hussain'\n",
    "\n",
    "from selenium import webdriver  \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function will login an provided account to linkedIn to use it in web browser.\n",
    "def login():\n",
    "    signInButton = driver.find_element_by_xpath('/html/body/nav/div/a[2]') # This will find SignIn Button on webBrowser.\n",
    "    signInButton.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "    email = driver.find_element_by_xpath('//*[@id=\"username\"]')           # This Will find the Username Text box\n",
    "    email.send_keys('m_nouman@hotmail.com')                               # Provided username will be sent to the TextBox\n",
    "\n",
    "    time.sleep(5)\n",
    "    passs = driver.find_element_by_xpath('//*[@id=\"password\"]')           # This Will find the Password Text box\n",
    "    passs.send_keys('wolfie_54321')                                       # Provided Password will be sent to the TextBox\n",
    "\n",
    "    login = driver.find_element_by_xpath('//*[@id=\"organic-div\"]/form/div[3]/button')   # This will again find the signIn button \n",
    "    login.click()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function will generate a new Link for everytime it will be called based on its parameters.\n",
    "def link_generator(country,typee,num):\n",
    "    f_l = 'https://www.linkedin.com/search/results/'\n",
    "    m_l = '/?keywords='\n",
    "    r_l = '&origin=SWITCH_SEARCH_VERTICAL&page='\n",
    "    l_l = '&sid=cZR'\n",
    "    return f_l + typee + m_l + country + r_l + str(num) + l_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This portion will Open a Chrome Webdriver and opens LinkedIn Page.\n",
    "driver = webdriver.Chrome(executable_path = \"C:\\\\Users\\m_nou\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe\")\n",
    "link = \"https://www.linkedin.com/\"\n",
    "driver.get(link)\n",
    "\n",
    "login() # After Opening WebDriver Login Function is called to login into provided account\n",
    "\n",
    "Country = \"Pakistan\"      # User can select any country \n",
    "typee = \"people\"          # it could be People or Companies\n",
    "no_of_profiles =  '100'   # user can also select the number of profiles he wanna scrabe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2 # used for turning pages\n",
    "j = 64 # used for changing search Keyword\n",
    "while(True):\n",
    "    if (i== 100):  # if pages reach 100 then Keyword will be changed and new 100 pages will load\n",
    "        Country = chr(j + 1)\n",
    "        j = j + 1\n",
    "        i = 2      # and i will again set to 2nd page and new link will be generated \n",
    "        link = link_generator(Country,typee,i)\n",
    "    else:\n",
    "        link = link_generator(Country,typee,i)\n",
    "    driver.get(link)\n",
    "    content = driver.page_source\n",
    "    main = BeautifulSoup(content,'html.parser')  # Here all the present page content will be soup into Main.\n",
    "    s = main.findAll('li',attrs = {'class' : 'reusable-search__result-container' })  # All Present profiles Div will be stored into \"s\"\n",
    "\n",
    "    # This for loop will take one profile from 's' and Extract data from each profile.\n",
    "    for profile in s:\n",
    "        pro = []\n",
    "        time.sleep(8)\n",
    "        pro_link = profile.find('a')['href']  # This will get Profile Link of the user and will open it in same driver.\n",
    "        driver.get(pro_link)\n",
    "        #print(pro_link)\n",
    "        # From here One by one All data wil be Scraped and if in some Profile Data is not present it will not cause error. It will just skip the entity.\n",
    "        sub = BeautifulSoup(driver.page_source,'html.parser') \n",
    "        data = sub.find('div',attrs = {'class' : 'ph5 pb5'})\n",
    "        if(data == None):\n",
    "            data = sub.find('div',attrs = {'class' : 'ph5 '})\n",
    "        try:\n",
    "            name = data.find('div',attrs = {'class' : 'mt2 relative'}).find('h1').text\n",
    "        except:\n",
    "            name = \"\"\n",
    "        try:\n",
    "            job = data.find('div',attrs = {'class' : 'mt2 relative'}).find('div',attrs = {'class' : 'text-body-medium break-words'}).text.split(\"\\n\")[1]\n",
    "        except:\n",
    "            job = \"\"\n",
    "        try:\n",
    "            comp = data.find('div',attrs = {'class' : 'mt2 relative'}).find('ul',attrs = {'class': 'pv-text-details__right-panel'}).find('li',attrs = {'class': 'pv-text-details__right-panel-item'}).find('div').text.split(\"\\n\")[2]\n",
    "        except:\n",
    "            comp = \"\"\n",
    "        try:\n",
    "            address = data.find('div',attrs = {'class' : 'mt2 relative'}).find('div',attrs = {'class': 'pb2 pv-text-details__left-panel'}).find('span').text.split(\"\\n\")[1]\n",
    "            try:\n",
    "                country = address.split(\",\")[len(address.split(\",\"))-1]\n",
    "            except:\n",
    "                country = Country\n",
    "        except:\n",
    "            address = \"\"\n",
    "            country = Country\n",
    "        try:\n",
    "            connections = data.find('ul',attrs = {'class':'pv-top-card--list pv-top-card--list-bullet display-flex pb1'}).find('li',attrs = {'class':'text-body-small'}).text.split(\" \")[0].split(\"\\n\")[2]\n",
    "        except:\n",
    "            connections = \"\"\n",
    "        try:\n",
    "            username = pro_link.split(\"/\")[4].split(\"?\")[0]\n",
    "        except:\n",
    "            username = \"\"\n",
    "        \"\"\"try:\n",
    "            print(\"try\")\n",
    "            #.find('span',attrs = {'class':'align-self-center t-14 t-black--light'})\n",
    "        except:\n",
    "            followers = \n",
    "        followers = sub.find('div',attrs = {'class' : 'pv-deferred-area ember-view'}).find('section',attrs = {'class':'pv-profile-section pv-recent-activity-section-v2 artdeco-card p5 mt4 ember-view'}).text \"\"\"\n",
    "        \n",
    "        # Here Data is being stored into List. Strip function is used to remove any extra spaces which is sometimes caused.\n",
    "        pro.append(name.strip())\n",
    "        pro.append(job.strip())\n",
    "        pro.append(comp.strip())\n",
    "        pro.append(address.strip())\n",
    "        pro.append(country.strip())\n",
    "        pro.append(connections.strip())\n",
    "        pro.append(username.strip())\n",
    "        if(name.strip() != \"\"):       #if someone's data is not found then it will not store empty data into CSv File.\n",
    "            print(pro)\n",
    "\n",
    "            # opening the csv file in 'a+' mode allows the user to save new data keeping the previous data as well.\n",
    "            # If utf-8 is not used a Error will be cause and it newline is not used then it will skip one line everytime while Storing list.\n",
    "            with open('profiles.csv', 'a+',encoding=\"utf-8\",newline='') as file:\n",
    "                myfile = csv.writer(file)\n",
    "                myfile.writerow(pro)\n",
    "        driver.back() # After Scraping a specific profile driver will be brought back to the pages.\n",
    "    i = i + 1         # increment for pages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
