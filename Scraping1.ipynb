{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61935186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name : 'Muhammad Nouman Butt' & 'Rafaqat Hussain'\n",
    "\n",
    "from selenium import webdriver\n",
    "from random import randrange\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def login():\n",
    "    signInButton = driver.find_element_by_xpath('/html/body/nav/div/a[2]') # This will find SignIn Button on webBrowser.\n",
    "    signInButton.click()\n",
    "    time.sleep(randrange(3,6))\n",
    "\n",
    "    email = driver.find_element_by_xpath('//*[@id=\"username\"]')           # This Will find the Username Text box\n",
    "    email.send_keys('any linkedIn Email')                               # Provided username will be sent to the TextBox\n",
    "\n",
    "    time.sleep(randrange(3,6))\n",
    "    passs = driver.find_element_by_xpath('//*[@id=\"password\"]')           # This Will find the Password Text box\n",
    "    passs.send_keys(\"linkedIn email's password\")                                       # Provided Password will be sent to the TextBox\n",
    "\n",
    "    login = driver.find_element_by_xpath('//*[@id=\"organic-div\"]/form/div[3]/button')   # This will again find the signIn button \n",
    "    login.click()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function will generate a new Link for everytime it will be called based on its parameters.\n",
    "def link_generator(country,typee,num):\n",
    "    f_l = 'https://www.linkedin.com/search/results/'\n",
    "    m_l = '/?keywords='\n",
    "    r_l = '&origin=SWITCH_SEARCH_VERTICAL&page='\n",
    "    l_l = '&sid=cZR'\n",
    "    return f_l + typee + m_l + country + r_l + str(num) + l_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6210a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This portion will Open a Chrome Webdriver and opens LinkedIn Page.\n",
    "driver = webdriver.Chrome(executable_path = 'C:\\\\Users\\m_nou\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe')\n",
    "link = \"https://www.linkedin.com/\"\n",
    "driver.get(link)\n",
    "\n",
    "login() # After Opening WebDriver Login Function is called to login into provided account\n",
    "\n",
    "Country = \"Pakistan\"      # User can select any country \n",
    "typee = \"people\"          # it could be People or Companies\n",
    "no_of_profiles =  '100'   # user can also select the number of profiles he wanna scrabe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3651bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "link = \"https://www.bing.com/search?q=site%3Alinkedin.com%2Fin%2F+and+n&qs=n&form=QBRE&sp=-1&pq=site%3Alinkedin.com%2Fin%2F+and+&sc=0-26&sk=&cvid=7C2F51DE0D404173AC209623D9592205\"\n",
    "\n",
    "while(True):\n",
    "    time.sleep(randrange(3,15))\n",
    "    driver.get(link)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)  # Here all the present page content will be soup into Main.\n",
    "    profiles = soup.findAll('li',attrs = {'class': 'b_algo'}) # All Present profiles Div will be stored into \"profiles\"\n",
    "    \n",
    "    # This for loop will take one profile from 's' and Extract data from each profile.\n",
    "    for pro in profiles:\n",
    "        proo = []\n",
    "        pro_link = pro.find('h2').find('a')[\"href\"]  # This will get Profile Link of the user and will open it in same driver.\n",
    "        time.sleep(randrange(3,12))\n",
    "        \n",
    "        #print(pro_link)\n",
    "        driver.get(pro_link)\n",
    "        time.sleep(randrange(2,4))\n",
    "        \n",
    "        # From here One by one All data wil be Scraped and if in some Profile Data is not present it will not cause error. It will just skip the entity.\n",
    "        sub = BeautifulSoup(driver.page_source) \n",
    "        data = sub.find('div',attrs = {'class' : 'ph5 pb5'})\n",
    "        if(data == None):\n",
    "            data = sub.find('div',attrs = {'class' : 'ph5 '})\n",
    "        try:\n",
    "            name = data.find('div',attrs = {'class' : 'mt2 relative'}).find('h1',attrs = {'class':'text-heading-xlarge inline t-24 v-align-middle break-words'}).text\n",
    "        except:\n",
    "            name = \"\"\n",
    "        try:\n",
    "            job = data.find('div',attrs = {'class' : 'mt2 relative'}).find('div',attrs = {'class' : 'text-body-medium break-words'}).text\n",
    "        except:\n",
    "            job = \"\"\n",
    "        try:\n",
    "            comp = data.find('div',attrs = {'class' : 'mt2 relative'}).find('ul',attrs = {'class': 'pv-text-details__right-panel'}).find('li',attrs = {'class': 'pv-text-details__right-panel-item'}).find('div').text\n",
    "        except:\n",
    "            comp = \"\"\n",
    "        try:\n",
    "            address = data.find('div',attrs = {'class' : 'mt2 relative'}).find('div',attrs = {'class': 'pb2 pv-text-details__left-panel'}).find('span').text\n",
    "            try:\n",
    "                country = address.split(\",\")[len(address.split(\",\"))-1]\n",
    "            except:\n",
    "                country = Country\n",
    "        except:\n",
    "            address = \"\"\n",
    "            country = Country\n",
    "        try:\n",
    "            connections = data.find('ul',attrs = {'class':'pv-top-card--list pv-top-card--list-bullet display-flex pb1'}).find('li',attrs = {'class':'text-body-small'}).find('span',attrs = {'class':'t-bold'}).text\n",
    "        except:\n",
    "            connections = \"\"\n",
    "        try:\n",
    "            username = pro_link.split(\"/\")[4]\n",
    "        except:\n",
    "            username = \"\"\n",
    "            \n",
    "        # Here Data is being stored into List. Strip function is used to remove any extra spaces which is sometimes caused.\n",
    "        proo.append(name.strip())\n",
    "        proo.append(job.strip())\n",
    "        proo.append(comp.strip())\n",
    "        proo.append(address.strip())\n",
    "        proo.append(country.strip())\n",
    "        proo.append(connections.strip())\n",
    "        proo.append(username.strip())\n",
    "        if(name.strip() != \"\"):       #if someone's data is not found then it will not store empty data into CSv File.\n",
    "            #print(proo)\n",
    "\n",
    "            # opening the csv file in 'a+' mode allows the user to save new data keeping the previous data as well.\n",
    "            # If utf-8 is not used a Error will be cause and it newline is not used then it will skip one line everytime while Storing list.\n",
    "            with open('profiles.csv', 'a+',encoding=\"utf-8\",newline='') as file:\n",
    "                myfile = csv.writer(file)\n",
    "                myfile.writerow(proo)\n",
    "    driver.back()\n",
    "    s = soup.find('li',attrs = {'class': 'b_pag'}).find('a',attrs = {'class':'sb_pagN sb_pagN_bp b_widePag sb_bp'})[\"href\"]\n",
    "    link = \"https://www.bing.com\" + s\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0649b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951df8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
